{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI and ML - group project\n",
    "\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing dataset\n",
    "df = pd.read_csv(r'C:/Users/silva/Documents/Faculdade/IntercÃ¢mbio/AI and Machine Learning/Group project/customer_segmentation.csv')\n",
    "print(df)\n",
    "\n",
    "# Fixing the date\n",
    "datelist= ['order_purchase_timestamp','order_approved_at','order_approved_at','order_delivered_carrier_date','order_delivered_customer_date','order_estimated_delivery_date','shipping_limit_date']\n",
    "\n",
    "# Converting the list of date variables to a pandas datetime object\n",
    "for c in datelist:\n",
    "    df[c]=pd.to_datetime(df[c])\n",
    "\n",
    "# Separating the date and time in the order column they might be interesting variables for the analysis\n",
    "df['order_date'] = [d.date() for d in df['order_purchase_timestamp']]\n",
    "df['order_time'] = [d.time() for d in df['order_purchase_timestamp']]\n",
    "\n",
    "# Taking a look at the data structure\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe()\n",
    "df.shape\n",
    "\n",
    "# Data preparation\n",
    "\n",
    "# Removing duplicates and resetting the index of the dataframe\n",
    "df.duplicated().sum()\n",
    "df.drop_duplicates(keep = 'first', inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Checking for missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Checking for outliers\n",
    "plt.figure()\n",
    "df.reset_index().plot(kind='scatter', x='index', y='payment_installments')\n",
    "plt.figure()\n",
    "df.reset_index().plot(kind='scatter', x='index', y='payment_value')\n",
    "\n",
    "# Checking the mean and standar deviation of the payment value (to check how far is the outlier observed in the scatterplot) \n",
    "mean_pv = df[\"payment_value\"].mean() # mean of 195.2\n",
    "std_pv = df[\"payment_value\"].std() # std of 295.5\n",
    "\n",
    "# Seeing how many outliers there are, the output prints 5 payments higher than 4000\n",
    "# Value of 4000 decided by looking at the scatterplot\n",
    "outlier1 = df[df['payment_value'] > 4000]\n",
    "print('\\nOutlier dataframe:\\n', outlier1)\n",
    "\n",
    "# Deleting 5 outliers out of 13718 other orders and resetting the index again\n",
    "df = df.drop(df[df.payment_value > 4000].index)\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "# Continuing to look at other variables by plotting scatterplots\n",
    "plt.figure()\n",
    "df.reset_index().plot(kind='scatter', x='index', y='payment_value') # re-plotting after removing outliers\n",
    "plt.figure()\n",
    "df.reset_index().plot(kind='scatter', x='index', y='price')\n",
    "plt.figure()\n",
    "df.reset_index().plot(kind='scatter', x='index', y='freight_value')\n",
    "plt.figure()\n",
    "\n",
    "# Creating a correlation matrix heatmap to look at the relation between variables\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.figure()\n",
    "\n",
    "# Counting customers per city and per state and creating dataframes with the purpose of plotting a graph\n",
    "customerstate = df.groupby('customer_state').count()['customer_id'].reset_index()\n",
    "customercity= df.groupby('customer_city').count()['customer_id'].reset_index()\n",
    "\n",
    "# Graph of numer of customers in each state\n",
    "plt.figure(figsize = (25,7))\n",
    "plt.subplot(121)\n",
    "sns.barplot(data = customerstate.sort_values('customer_id', ascending = False), x = 'customer_state', y = 'customer_id')\n",
    "plt.title('Number of Customers in a State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Number of Customers')\n",
    "\n",
    "# Graph of top 10 cities with most customers\n",
    "plt.subplot(122)\n",
    "sns.barplot(data = customercity.sort_values('customer_id', ascending = False).nlargest(10,'customer_id'), x = 'customer_id', y = 'customer_city')\n",
    "plt.title('Cities with more Customers')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Number of Customers');\n",
    "\n",
    "# Grouping similar categories of product category names in case we use this variable in future analysis (encoding becomes easier, there would be less dummies)\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['art', 'arts_and_craftmanship', 'sports_leisure',\n",
    "                                                                                   'garden_tools', 'flowers', 'music', 'musical_instruments',\n",
    "                                                                                   'books_general_interest', 'books_imported', 'books_technical'], 'hobbies')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['air_conditioning', 'bed_bath_table', 'furniture_bedroom',\n",
    "                                                                                   'furniture_decor', 'furniture_living_room', 'home_appliances',\n",
    "                                                                                   'home_appliances_2', 'home_comfort_2', 'home_confort',\n",
    "                                                                                   'home_construction', 'housewares', 'kitchen_dining_laundry_garden_furniture',\n",
    "                                                                                   'small_appliances', 'small_appliances_home_oven_and_coffee', 'office_furniture',\n",
    "                                                                                   'signaling_and_security', 'stationery', 'luggage_accessories'], 'home_products')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['drinks', 'food', 'food_drink'], 'food_drink')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['construction_tools_construction', 'construction_tools_lights',\n",
    "                                                                                   'construction_tools_safety', 'costruction_tools_garden', \n",
    "                                                                                   'costruction_tools_tools'], 'construction_tools')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['audio', 'auto', 'cds_dvds_musicals', 'cine_photo', 'computers', \n",
    "                                                                                   'computers_accessories', 'consoles_games', 'dvds_blu_ray', 'electronics',\n",
    "                                                                                   'fixed_telephony', 'telephony', 'tablets_printing_image'], 'electronic_gadgets')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['fashion_bags_accessories', 'fashion_childrens_clothes', 'fashion_male_clothing',\n",
    "                                                                                   'fashion_shoes', 'fashion_underwear_beach', 'health_beauty', 'perfumery'], 'fashion_beauty')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['baby', 'diapers_and_hygiene', 'toys', 'party_supplies', 'pet_shop',\n",
    "                                                                                   'christmas_supplies', 'cool_stuff', 'watches_gifts'], 'family_festivities')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['industry_commerce_and_business', 'market_place'], 'sellers')\n",
    "\n",
    "\n",
    "# Counting the product categories to check which one is most and less ordered\n",
    "best_seller_p = df['product_category_name_english'].value_counts().reset_index().nlargest(5,'product_category_name_english')\n",
    "worst_seller_p = df['product_category_name_english'].value_counts().reset_index().nsmallest(5,'product_category_name_english')\n",
    "\n",
    "# Graphs aesthetic\n",
    "plt.figure(figsize = (15,12))\n",
    "green_color = sns.color_palette()[3]\n",
    "red_color = sns.color_palette()[2]\n",
    "\n",
    "# Graph of top 10 most ordered products\n",
    "plt.subplot(211)\n",
    "sns.barplot(data = best_seller_p, x = 'product_category_name_english', y = 'index', color = green_color)\n",
    "plt.title('Top 5 Product Categories Ordered')\n",
    "plt.xlabel('Number of Orders')\n",
    "plt.ylabel('Product Category');\n",
    "\n",
    "# Graph of top 10 less ordered products\n",
    "plt.subplot(212)\n",
    "sns.barplot(data = worst_seller_p, x = 'product_category_name_english', y = 'index', color =red_color )\n",
    "plt.title('Lowest 5 Product Categories Ordered')\n",
    "plt.xlabel('Number of Orders')\n",
    "plt.ylabel('Product Category');\n",
    "\n",
    "# Counting different payment types to plot which ones are more used\n",
    "payments_types = df['payment_type'].value_counts().reset_index()\n",
    "\n",
    "plt.figure(figsize = (25,7))\n",
    "\n",
    "# Graph of number of orders per payment type\n",
    "plt.subplot(121)\n",
    "sns.barplot(data = payments_types, x = 'index', y = 'payment_type')\n",
    "plt.title('Orders by Payment type')\n",
    "plt.xlabel('Payment Type')\n",
    "plt.ylabel('Number of Orders');\n",
    "\n",
    "# Graph of number of orders per number of payment installments\n",
    "plt.subplot(122)\n",
    "sns.barplot(data = df['payment_installments'].value_counts().reset_index(), x = 'index', y = 'payment_installments')\n",
    "plt.title('Count of Orders With Number of Payment Installments')\n",
    "plt.xlabel('Number of Payment Installments')\n",
    "plt.ylabel('Count of Orders');\n",
    "\n",
    "# Encoding variables \n",
    "df = pd.get_dummies(df,prefix=['customer_state: '], columns = ['customer_state'], drop_first=True)\n",
    "df = pd.get_dummies(df,prefix=['payment_type: '], columns = ['payment_type'], drop_first=True)\n",
    "df = pd.get_dummies(df,prefix=['seller_state: '], columns = ['seller_state'], drop_first=True)\n",
    "df = pd.get_dummies(df,prefix=['product_category_name: '], columns = ['product_category_name_english'], drop_first=True)\n",
    "\n",
    "# Calculating the RFM\n",
    "\n",
    "# Recency = time since a customer's last purchase\n",
    "# Calculating each purchasing time stamp minus the most recent purchase timestamp (max)\n",
    "df['recency'] = max(df['order_purchase_timestamp'])-df['order_purchase_timestamp']\n",
    "# Getting the minimum recency value for each customer (customer with multiple purchases ended up with multiple recencies, therefore calculating the most recent purchase)\n",
    "df['recency'] = df.groupby(['customer_unique_id'], as_index=False)['recency'].transform('min')\n",
    "# Keeping only the days in the column, leaving out the time\n",
    "df['recency'] = df['recency'].dt.days\n",
    "\n",
    "# Frequency = how many times has a customer made a purchase\n",
    "# Counting purchases per customer unique id and adding values to a new column in the dataset\n",
    "df['frequency'] = df.groupby(['customer_unique_id'], as_index=False)['order_id'].transform('count')\n",
    "\n",
    "# Monetary = total amount a customer has spend purchasing products\n",
    "# Calculating it by summing all the payment values a customer has spent\n",
    "df['monetary'] = df.groupby(['customer_unique_id'], as_index=False)['payment_value'].transform('sum')\n",
    "\n",
    "# Creating an empty column to calculate recency score\n",
    "df['recency_score'] = ''\n",
    "\n",
    "# For function that inputs the score in the empty column based on the conditions set\n",
    "# The numbers were chosen based of the recency mean, std, max and min + our reasoning considered aspects like \"how many months could we consider the customer inactive?\"\n",
    "print('recency mean: ', df['recency'].mean())\n",
    "print('recency std: ', df['recency'].std())\n",
    "print('recency max: ', df['recency'].max())\n",
    "print('recency min: ', df['recency'].min())\n",
    "\n",
    "for i in df.index:\n",
    "    if df['recency'][i] <= 30:\n",
    "        df['recency_score'][i] = 5\n",
    "    elif (df['recency'][i] > 30) and (df['recency'][i] <= 60):\n",
    "        df['recency_score'][i] = 4\n",
    "    elif (df['recency'][i] > 60) and (df['recency'][i] <= 120):\n",
    "        df['recency_score'][i] = 3\n",
    "    elif (df['recency'][i] > 120) and (df['recency'][i] <= 180):\n",
    "        df['recency_score'][i] = 2\n",
    "    elif (df['recency'][i] > 180):\n",
    "        df['recency_score'][i] = 1\n",
    "\n",
    "# Creating an empty column to calculate frequency score\n",
    "df['frequency_score'] = ''\n",
    "\n",
    "# For function that inputs the score in the empty column based on the conditions set\n",
    "# The numbers were chosen based of the recency mean, std, max and min + our reasoning considered aspects like \"most customers purchased only one time, and max purchases were 13, how can we score from 0 to above\"\n",
    "print('frequency mean: ', df['frequency'].mean())\n",
    "print('frequency std: ', df['frequency'].std())\n",
    "print('frequency max: ', df['frequency'].max())\n",
    "print('frequency min: ', df['frequency'].min())\n",
    "\n",
    "for i in df.index:\n",
    "    if df['frequency'][i] >= 10:\n",
    "        df['frequency_score'][i] = 5\n",
    "    elif (df['frequency'][i] >= 6) and (df['frequency'][i] < 10):\n",
    "        df['frequency_score'][i] = 4\n",
    "    elif (df['frequency'][i] >= 4) and (df['frequency'][i] < 6):\n",
    "        df['frequency_score'][i] = 3\n",
    "    elif (df['frequency'][i] >= 2) and (df['frequency'][i] < 4):\n",
    "        df['frequency_score'][i] = 2\n",
    "    elif (df['frequency'][i] == 1):\n",
    "        df['frequency_score'][i] = 1\n",
    "\n",
    "# Creating an empty column to calculate monetary score        \n",
    "df['monetary_score'] = ''\n",
    "\n",
    "# For function that inputs the score in the empty column based on the conditions set\n",
    "# The numbers were chosen based of the recency mean, std, max and min + our reasoning considered aspects like \"the mean of the payment was 395.1, and the std is 1090.5, how can we score the customers based on spending\"\n",
    "print('monetary mean: ', df['monetary'].mean())\n",
    "print('monetary std: ', df['monetary'].std())\n",
    "print('monetary max: ', df['monetary'].max())\n",
    "print('monetary min: ', df['monetary'].min())\n",
    "\n",
    "for i in df.index:\n",
    "    if df['monetary'][i] > 500:\n",
    "        df['monetary_score'][i] = 5\n",
    "    elif (df['monetary'][i] > 250) and (df['monetary'][i] <= 500):\n",
    "        df['monetary_score'][i] = 4\n",
    "    elif (df['monetary'][i] > 150) and (df['monetary'][i] <= 250):\n",
    "        df['monetary_score'][i] = 3\n",
    "    elif (df['monetary'][i] > 100) and (df['monetary'][i] <= 150):\n",
    "        df['monetary_score'][i] = 2\n",
    "    elif (df['monetary'][i] <= 100):\n",
    "        df['monetary_score'][i] = 1\n",
    "        \n",
    "# Uniting the scores in the same columns to find segments such as 555 (5 score for all rfm)\n",
    "df['rfm_segment'] = df['recency_score'].astype(str) + df['frequency_score'].astype(str) + df['monetary_score'].astype(str)\n",
    "\n",
    "# Turning all numbers from last created columns to numeric variables to use in future analysis\n",
    "df['rfm_segment'] = pd.to_numeric(df['rfm_segment'])\n",
    "df['recency_score'] = pd.to_numeric(df['recency_score'])\n",
    "df['frequency_score'] = pd.to_numeric(df['frequency_score'])\n",
    "df['monetary_score'] = pd.to_numeric(df['monetary_score'])\n",
    "\n",
    "# Creating a dataframe with the rfm variables that are going to be used in the clustering algorithms\n",
    "rfm = df[['recency','frequency','monetary', 'recency_score', 'frequency_score','monetary_score']]\n",
    "\n",
    "# Standardizing the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "rfm_std = scaler.fit_transform(rfm)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "# K means\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score \n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "\n",
    "import plotly as py\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# Using eblow method to find out optimal k value with WCSS score\n",
    "\n",
    "WCSS = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(rfm_std)\n",
    "    WCSS.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), WCSS, marker='o', label='line with marker')\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Fitting K-Means to the dataset\n",
    "# Use fit_predict to cluster the dataset\n",
    "kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(rfm_std)\n",
    "\n",
    "# Visualising the clusters\n",
    "plt.scatter(rfm_std[y_kmeans == 0, 0], rfm_std[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(rfm_std[y_kmeans == 1, 0], rfm_std[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(rfm_std[y_kmeans == 2, 0], rfm_std[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\n",
    "plt.title('Clusters of customers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate cluster validation metrics\n",
    "score_kemans_s = silhouette_score(rfm_std,kmeans.labels_, metric='euclidean')\n",
    "score_kemans_c = calinski_harabasz_score(rfm_std, kmeans.labels_)\n",
    "score_kemans_d = davies_bouldin_score(rfm_std, y_kmeans)\n",
    "print('Silhouette Score: %.3f' % score_kemans_s)\n",
    "print('Calinski Harabasz Score: %.3f' % score_kemans_c)\n",
    "print('Davies Bouldin Score: %.3f' % score_kemans_d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# hierarchical clustering\n",
    "#Dendrogram for Hierarchical Clustering\n",
    "#â¥Using the dendrogram to find the optimal number of clusters\n",
    "\n",
    "import scipy.cluster.hierarchy as sch\n",
    "dendrogram = sch.dendrogram(sch.linkage(rfm_std, method = 'ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()\n",
    "\n",
    "# Fitting Hierarchical Clustering to the dataset\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\n",
    "y_hc = hc.fit_predict(rfm_std)\n",
    "\n",
    "\n",
    "\n",
    "# Agglomerative clustering\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# define the model\n",
    "\n",
    "# retrieve unique clusters\n",
    "clusters = unique(y_hc)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate cluster validation metrics\n",
    "score_AGclustering_s = silhouette_score(rfm_std, hc.labels_ , metric='euclidean')\n",
    "score_AGclustering_c = calinski_harabasz_score(rfm_std, hc.labels_ ,)\n",
    "score_AGclustering_d = davies_bouldin_score(rfm_std, y_hc)\n",
    "print('Silhouette Score: %.3f' % score_AGclustering_s)\n",
    "print('Calinski Harabasz Score: %.3f' % score_AGclustering_c)\n",
    "print('Davies Bouldin Score: %.3f' % score_AGclustering_d)\n",
    "\n",
    "\n",
    "\n",
    "# Visualising the clusters\n",
    "#plt.scatter(rfm_std[y_hc == 0, 0], rfm_std[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "#plt.scatter(rfm_std[y_hc == 1, 0], rfm_std[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "#plt.scatter(rfm_std[y_hc == 2, 0], rfm_std[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "#plt.scatter(rfm_std[y_hc == 3, 0], rfm_std[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n",
    "#plt.scatter(rfm_std[y_hc == 4, 0], rfm_std[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n",
    "#plt.scatter(X[y_hc == 5, 0], X[y_hc == 5, 1], s = 100, c = 'black', label = 'Cluster 6')\n",
    "#plt.scatter(X[y_hc == 6, 0], X[y_hc == 6, 1], s = 100, c = 'orange', label = 'Cluster 7')\n",
    "#plt.title('Clusters of customers')\n",
    "##plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#DBSCAN METHOD \n",
    "#Unlike k-means, DBSCAN will figure out the number of clusters.\n",
    "#DBSCAN works by determining whether the minimum number of points are close enough\n",
    "#to one another to be considered part of a single cluster. \n",
    "#DBSCAN is very sensitive to scale since epsilon is a fixed value for the maximum distance between two points.\n",
    "#In laymanâs terms,\n",
    "# we find a suitable value for epsilon by calculating the distance to the nearest n points for each point,\n",
    "# sorting and plotting the results. Then we look to see where the change is most pronounced\n",
    "# (think of the angle between your arm and forearm) and select that as epsilon.\n",
    "#We can calculate the distance from each point to its closest neighbour using the NearestNeighbors.\n",
    "# The point itself is included in n_neighbors. The kneighbors method returns two arrays, \n",
    "#one which contains the distance to the closest n_neighbors points and the other which contains the index for each of those points.\n",
    "\n",
    "rfm_std_df = pd.DataFrame(rfm_std, columns = ['recency','frequency','monetary', 'recency_score', 'frequency_score','monetary_score'])\n",
    "\n",
    "# parameter tuning for eps\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=11)\n",
    "neighbors = nearest_neighbors.fit(rfm_std)\n",
    "distances, indices = neighbors.kneighbors(rfm_std)\n",
    "distances = np.sort(distances[:,10], axis=0)\n",
    "from kneed import KneeLocator\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "print(distances[knee.knee])\n",
    "\n",
    "\n",
    "#after printg this we know the optimal epsilon is 0.16516090603780492\n",
    "#so we plug it the next code\n",
    "\n",
    "# dbscan clustering\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "# define the model\n",
    "# rule of thumb for min_samples: 2*len(cluster_df.columns)\n",
    "min_sample = 2*len(rfm_std.columns)\n",
    "print(min_sample)\n",
    "model = DBSCAN(eps=0.16516090603780492, min_samples= 12)\n",
    "\n",
    "# fit model and predict clusters\n",
    "yhat = model.fit_predict(rfm_std)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "print(clusters)\n",
    "# Calculate cluster validation metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dbscan( rfm_std, eps=0.16516090603780492, min_samples=12):\n",
    "    ss = StandardScaler()\n",
    "    agg = ss.fit_transform(rfm_std)\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    db.fit(rfm_std)\n",
    "    y_pred = db.fit_predict(rfm_std)\n",
    "    plt.scatter(rfm_std[:,0], rfm_std[:,1],c=y_pred, cmap='Paired')\n",
    "    plt.title(\"DBSCAN\")\n",
    "    plt.plot()\n",
    "#dbscan(agg, eps=.5, min_samples=5)\n",
    "\n",
    "\n",
    "# cluster the data into five clusters\n",
    "dbscan = DBSCAN(eps =0.16516090603780492 , min_samples = 12).fit(rfm_std) # fitting the model\n",
    "labels = dbscan.labels_ # getting the labels\n",
    "# Plot the clusters\n",
    "plt.scatter(rfm_std[:, 0], rfm_std[:,1], c = labels, cmap= \"plasma\") # plotting the clusters\n",
    "plt.title('clusters whith dbscan')\n",
    "plt.xlabel(\"\") # X-axis label\n",
    "plt.ylabel(\"\") # Y-axis label\n",
    "plt.show() # showing the plot\n",
    "\n",
    "\n",
    "\n",
    "# Calculate cluster validation metrics\n",
    "score_dbsacn_s = silhouette_score(rfm_std, yhat, metric='euclidean')\n",
    "score_dbsacn_c = calinski_harabasz_score(rfm_std, yhat)\n",
    "score_dbsacn_d = davies_bouldin_score(rfm_std, yhat)\n",
    "print('Silhouette Score: %.4f' % score_dbsacn_s)\n",
    "print('Calinski Harabasz Score: %.4f' % score_dbsacn_c)\n",
    "print('Davies Bouldin Score: %.4f' % score_dbsacn_d)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c2bd8370d9835a4a8706f1c8ec7394e0db6556a7b06b35ae493fb0b20d49980"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
