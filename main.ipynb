{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI and ML - group project\n",
    "\n",
    "# Importing libraries\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing dataset\n",
    "df = pd.read_csv(\n",
    "    r'C:/Users/silva/Documents/Faculdade/IntercÃ¢mbio/AI and Machine Learning/Group project/customer_segmentation.csv')\n",
    "print(df)\n",
    "\n",
    "# Fixing the date\n",
    "datelist = ['order_purchase_timestamp', 'order_approved_at', 'order_approved_at', 'order_delivered_carrier_date',\n",
    "            'order_delivered_customer_date', 'order_estimated_delivery_date', 'shipping_limit_date']\n",
    "\n",
    "# Converting the list of date variables to a pandas datetime object\n",
    "for c in datelist:\n",
    "    df[c] = pd.to_datetime(df[c])\n",
    "\n",
    "# Separating the date and time in the order column they might be interesting variables for the analysis\n",
    "df['order_date'] = [d.date() for d in df['order_purchase_timestamp']]\n",
    "df['order_time'] = [d.time() for d in df['order_purchase_timestamp']]\n",
    "\n",
    "# Taking a look at the data structure\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe()\n",
    "df.shape\n",
    "\n",
    "# Data preparation\n",
    "\n",
    "# Removing duplicates and resetting the index of the dataframe\n",
    "df.duplicated().sum()\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Checking for missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Checking for outliers\n",
    "plt.figure()\n",
    "df.reset_index().plot(kind='scatter', x='index',\n",
    "                      y='payment_installments', c='brown')\n",
    "plt.figure()\n",
    "df.reset_index().plot(kind='scatter', x='index', y='payment_value', c='gray')\n",
    "\n",
    "# Checking the mean and standar deviation of the payment value (to check how far is the outlier observed in the scatterplot)\n",
    "mean_pv = df[\"payment_value\"].mean()  # mean of 195.2\n",
    "std_pv = df[\"payment_value\"].std()  # std of 295.5\n",
    "\n",
    "# Seeing how many outliers there are, the output prints 5 payments higher than 4000\n",
    "# Value of 4000 decided by looking at the scatterplot\n",
    "outlier1 = df[df['payment_value'] > 4000]\n",
    "print('\\nOutlier dataframe:\\n', outlier1)\n",
    "\n",
    "# Deleting 5 outliers out of 13718 other orders and resetting the index again\n",
    "df = df.drop(df[df.payment_value > 4000].index)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Continuing to look at other variables by plotting scatterplots\n",
    "plt.figure()\n",
    "df.reset_index().plot(kind='scatter', x='index', y='payment_value',\n",
    "                      c='gray')  # re-plotting after removing outliers\n",
    "plt.figure()\n",
    "df.reset_index().plot(kind='scatter', x='index', y='price', c='orange')\n",
    "plt.figure()\n",
    "df.reset_index().plot(kind='scatter', x='index', y='freight_value', c='purple')\n",
    "plt.figure()\n",
    "\n",
    "# Creating a correlation matrix heatmap to look at the relation between variables\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.figure()\n",
    "\n",
    "# Counting customers per city and per state and creating dataframes with the purpose of plotting a graph\n",
    "customerstate = df.groupby('customer_state').count()[\n",
    "    'customer_id'].reset_index()\n",
    "customercity = df.groupby('customer_city').count()['customer_id'].reset_index()\n",
    "\n",
    "# Graph of numer of customers in each state\n",
    "plt.figure()\n",
    "sns.barplot(data=customerstate.sort_values('customer_id',\n",
    "            ascending=False), x='customer_state', y='customer_id')\n",
    "plt.title('Number of Customers per State', fontweight='bold')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Number of Customers')\n",
    "\n",
    "# Graph of top 10 cities with most customers\n",
    "plt.figure()\n",
    "sns.barplot(data=customercity.sort_values('customer_id', ascending=False).nlargest(\n",
    "    10, 'customer_id'), x='customer_id', y='customer_city')\n",
    "plt.title('Cities with more Customers', fontweight='bold')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Number of Customers')\n",
    "\n",
    "# Grouping similar categories of product category names in case we use this variable in future analysis (encoding becomes easier, there would be less dummies)\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['art', 'arts_and_craftmanship', 'sports_leisure',\n",
    "                                                                                   'garden_tools', 'flowers', 'music', 'musical_instruments',\n",
    "                                                                                   'books_general_interest', 'books_imported', 'books_technical'], 'hobbies')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['air_conditioning', 'bed_bath_table', 'furniture_bedroom',\n",
    "                                                                                   'furniture_decor', 'furniture_living_room', 'home_appliances',\n",
    "                                                                                   'home_appliances_2', 'home_comfort_2', 'home_confort',\n",
    "                                                                                   'home_construction', 'housewares', 'kitchen_dining_laundry_garden_furniture',\n",
    "                                                                                   'small_appliances', 'small_appliances_home_oven_and_coffee', 'office_furniture',\n",
    "                                                                                   'signaling_and_security', 'stationery', 'luggage_accessories'], 'home_products')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(\n",
    "    ['drinks', 'food', 'food_drink'], 'food_drink')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['construction_tools_construction', 'construction_tools_lights',\n",
    "                                                                                   'construction_tools_safety', 'costruction_tools_garden',\n",
    "                                                                                   'costruction_tools_tools'], 'construction_tools')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['audio', 'auto', 'cds_dvds_musicals', 'cine_photo', 'computers',\n",
    "                                                                                   'computers_accessories', 'consoles_games', 'dvds_blu_ray', 'electronics',\n",
    "                                                                                   'fixed_telephony', 'telephony', 'tablets_printing_image'], 'electronic_gadgets')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['fashion_bags_accessories', 'fashion_childrens_clothes', 'fashion_male_clothing',\n",
    "                                                                                   'fashion_shoes', 'fashion_underwear_beach', 'health_beauty', 'perfumery'], 'fashion_beauty')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(['baby', 'diapers_and_hygiene', 'toys', 'party_supplies', 'pet_shop',\n",
    "                                                                                   'christmas_supplies', 'cool_stuff', 'watches_gifts'], 'family_festivities')\n",
    "df['product_category_name_english'] = df['product_category_name_english'].replace(\n",
    "    ['industry_commerce_and_business', 'market_place'], 'sellers')\n",
    "\n",
    "\n",
    "# Counting the product categories to check which one is most and less ordered\n",
    "best_seller_p = df['product_category_name_english'].value_counts(\n",
    ").reset_index().nlargest(5, 'product_category_name_english')\n",
    "worst_seller_p = df['product_category_name_english'].value_counts(\n",
    ").reset_index().nsmallest(5, 'product_category_name_english')\n",
    "\n",
    "# Graphs aesthetic\n",
    "plt.figure(figsize=(15, 12))\n",
    "green_color = sns.color_palette()[3]\n",
    "red_color = sns.color_palette()[2]\n",
    "\n",
    "# Graph of top 10 most ordered products\n",
    "plt.subplot(211)\n",
    "sns.barplot(data=best_seller_p, x='product_category_name_english',\n",
    "            y='index', color=green_color)\n",
    "plt.title('Top 5 Product Categories Ordered', fontweight='bold')\n",
    "plt.xlabel('Number of Orders')\n",
    "plt.ylabel('Product Category')\n",
    "\n",
    "# Graph of top 10 less ordered products\n",
    "plt.subplot(212)\n",
    "sns.barplot(data=worst_seller_p, x='product_category_name_english',\n",
    "            y='index', color=red_color)\n",
    "plt.title('Lowest 5 Product Categories Ordered', fontweight='bold')\n",
    "plt.xlabel('Number of Orders')\n",
    "plt.ylabel('Product Category')\n",
    "\n",
    "# Counting different payment types to plot which ones are more used\n",
    "payments_types = df['payment_type'].value_counts().reset_index()\n",
    "\n",
    "plt.figure(figsize=(25, 7))\n",
    "\n",
    "# Graph of number of orders per payment type\n",
    "plt.subplot(121)\n",
    "sns.barplot(data=payments_types, x='index', y='payment_type')\n",
    "plt.title('Orders by Payment type', fontweight='bold')\n",
    "plt.xlabel('Payment Type')\n",
    "plt.ylabel('Number of Orders')\n",
    "\n",
    "# Graph of number of orders per number of payment installments\n",
    "plt.subplot(122)\n",
    "sns.barplot(data=df['payment_installments'].value_counts(\n",
    ").reset_index(), x='index', y='payment_installments')\n",
    "plt.title('Count of Orders With Number of Payment Installments', fontweight='bold')\n",
    "plt.xlabel('Number of Payment Installments')\n",
    "plt.ylabel('Count of Orders')\n",
    "\n",
    "# Encoding variables\n",
    "df = pd.get_dummies(df, prefix=['customer_state: '], columns=[\n",
    "                    'customer_state'], drop_first=False)\n",
    "df = pd.get_dummies(df, prefix=['payment_type: '], columns=[\n",
    "                    'payment_type'], drop_first=False)\n",
    "df = pd.get_dummies(df, prefix=['seller_state: '], columns=[\n",
    "                    'seller_state'], drop_first=False)\n",
    "df = pd.get_dummies(df, prefix=['product_category_name: '], columns=[\n",
    "                    'product_category_name_english'], drop_first=False)\n",
    "\n",
    "# Calculating the RFM\n",
    "\n",
    "# Recency = time since a customer's last purchase\n",
    "# Calculating each purchasing time stamp minus the most recent purchase timestamp (max)\n",
    "df['recency'] = max(df['order_purchase_timestamp']) - \\\n",
    "    df['order_purchase_timestamp']\n",
    "# Getting the minimum recency value for each customer (customer with multiple purchases ended up with multiple recencies, therefore calculating the most recent purchase)\n",
    "df['recency'] = df.groupby(['customer_unique_id'], as_index=False)[\n",
    "    'recency'].transform('min')\n",
    "# Keeping only the days in the column, leaving out the time\n",
    "df['recency'] = df['recency'].dt.days\n",
    "\n",
    "# Frequency = how many times has a customer made a purchase\n",
    "# Counting purchases per customer unique id and adding values to a new column in the dataset\n",
    "df['frequency'] = df.groupby(['customer_unique_id'], as_index=False)[\n",
    "    'order_id'].transform('count')\n",
    "\n",
    "# Monetary = total amount a customer has spend purchasing products\n",
    "# Calculating it by summing all the payment values a customer has spent\n",
    "df['monetary'] = df.groupby(['customer_unique_id'], as_index=False)[\n",
    "    'payment_value'].transform('sum')\n",
    "\n",
    "# Creating an empty column to calculate recency score\n",
    "df['recency_score'] = ''\n",
    "\n",
    "# For function that inputs the score in the empty column based on the conditions set\n",
    "# The numbers were chosen based of the recency mean, std, max and min + our reasoning considered aspects like \"how many months could we consider the customer inactive?\"\n",
    "print('recency mean: ', df['recency'].mean())\n",
    "print('recency std: ', df['recency'].std())\n",
    "print('recency max: ', df['recency'].max())\n",
    "print('recency min: ', df['recency'].min())\n",
    "\n",
    "for i in df.index:\n",
    "    if df['recency'][i] <= 30:\n",
    "        df['recency_score'][i] = 5\n",
    "    elif (df['recency'][i] > 30) and (df['recency'][i] <= 60):\n",
    "        df['recency_score'][i] = 4\n",
    "    elif (df['recency'][i] > 60) and (df['recency'][i] <= 120):\n",
    "        df['recency_score'][i] = 3\n",
    "    elif (df['recency'][i] > 120) and (df['recency'][i] <= 180):\n",
    "        df['recency_score'][i] = 2\n",
    "    elif (df['recency'][i] > 180):\n",
    "        df['recency_score'][i] = 1\n",
    "\n",
    "# Creating an empty column to calculate frequency score\n",
    "df['frequency_score'] = ''\n",
    "\n",
    "# For function that inputs the score in the empty column based on the conditions set\n",
    "# The numbers were chosen based of the recency mean, std, max and min + our reasoning considered aspects like \"most customers purchased only one time, and max purchases were 13, how can we score from 0 to above\"\n",
    "print('frequency mean: ', df['frequency'].mean())\n",
    "print('frequency std: ', df['frequency'].std())\n",
    "print('frequency max: ', df['frequency'].max())\n",
    "print('frequency min: ', df['frequency'].min())\n",
    "\n",
    "for i in df.index:\n",
    "    if df['frequency'][i] >= 10:\n",
    "        df['frequency_score'][i] = 5\n",
    "    elif (df['frequency'][i] >= 6) and (df['frequency'][i] < 10):\n",
    "        df['frequency_score'][i] = 4\n",
    "    elif (df['frequency'][i] >= 4) and (df['frequency'][i] < 6):\n",
    "        df['frequency_score'][i] = 3\n",
    "    elif (df['frequency'][i] >= 2) and (df['frequency'][i] < 4):\n",
    "        df['frequency_score'][i] = 2\n",
    "    elif (df['frequency'][i] == 1):\n",
    "        df['frequency_score'][i] = 1\n",
    "\n",
    "# Creating an empty column to calculate monetary score\n",
    "df['monetary_score'] = ''\n",
    "\n",
    "# For function that inputs the score in the empty column based on the conditions set\n",
    "# The numbers were chosen based of the recency mean, std, max and min + our reasoning considered aspects like \"the mean of the payment was 395.1, and the std is 1090.5, how can we score the customers based on spending\"\n",
    "print('monetary mean: ', df['monetary'].mean())\n",
    "print('monetary std: ', df['monetary'].std())\n",
    "print('monetary max: ', df['monetary'].max())\n",
    "print('monetary min: ', df['monetary'].min())\n",
    "\n",
    "for i in df.index:\n",
    "    if df['monetary'][i] > 500:\n",
    "        df['monetary_score'][i] = 5\n",
    "    elif (df['monetary'][i] > 250) and (df['monetary'][i] <= 500):\n",
    "        df['monetary_score'][i] = 4\n",
    "    elif (df['monetary'][i] > 150) and (df['monetary'][i] <= 250):\n",
    "        df['monetary_score'][i] = 3\n",
    "    elif (df['monetary'][i] > 100) and (df['monetary'][i] <= 150):\n",
    "        df['monetary_score'][i] = 2\n",
    "    elif (df['monetary'][i] <= 100):\n",
    "        df['monetary_score'][i] = 1\n",
    "\n",
    "# Uniting the scores in the same columns to find segments such as 555 (5 score for all rfm)\n",
    "df['rfm_segment'] = df['recency_score'].astype(\n",
    "    str) + df['frequency_score'].astype(str) + df['monetary_score'].astype(str)\n",
    "\n",
    "# Calculating total score (sum of all val)\n",
    "\n",
    "# Turning all numbers from last created columns to numeric variables to use in future analysis\n",
    "df['rfm_segment'] = pd.to_numeric(df['rfm_segment'])\n",
    "df['recency_score'] = pd.to_numeric(df['recency_score'])\n",
    "df['frequency_score'] = pd.to_numeric(df['frequency_score'])\n",
    "df['monetary_score'] = pd.to_numeric(df['monetary_score'])\n",
    "\n",
    "# Creating a dataframe with the rfm variables that are going to be used in the clustering algorithms\n",
    "rfm = df[['recency', 'frequency', 'monetary']]\n",
    "\n",
    "# Plotting a few exploratory graphs for rfm\n",
    "\n",
    "# Customers who order 1 time and 2 times are predominant. There fewer customers are the ones that have ordered more than 5 times.\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.distplot(df['frequency'], bins=8, kde=False, rug=True)\n",
    "plt.figure()\n",
    "\n",
    "# The ammount customers spend is frequently less than 300$.\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.distplot(df['monetary'], kde=False, rug=True)\n",
    "plt.xlim(0, 2000)\n",
    "\n",
    "# Customers who had come within the last 2 months are the majority and there are some customers that have not ordered for more than a year.\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.distplot(df['recency'], bins=8, kde=False, rug=True)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "rfm_std = scaler.fit_transform(rfm)\n",
    "\n",
    "# Segmenting the custumer in Gold, Silver and Bronze according to their scores in frequency, recency and monetary value\n",
    "# and keeping in mind that we assinged to recency a lower weight since in our opinion is a less impotant factor to determine future customer behavior.\n",
    "df['customer_category'] = ((df['monetary_score']*2) +\n",
    "                           (df['frequency_score']*2) + (df['recency_score']*1)) / 5\n",
    "\n",
    "for i in df.index:\n",
    "\n",
    "    if (df['customer_category'][i] >= 4) and (df['customer_category'][i] <= 5):\n",
    "        df['customer_category'][i] = 'Gold'\n",
    "    elif (df['customer_category'][i] > 2) and (df['customer_category'][i] < 4):\n",
    "        df['customer_category'][i] = 'Silver'\n",
    "    elif (df['customer_category'][i] >= 1) and (df['customer_category'][i] <= 2):\n",
    "        df['customer_category'][i] = 'Bronze'\n",
    "\n",
    "def generate_legend_labels(chart_data, chart_labels):\n",
    "    total_values = sum(chart_data.values)\n",
    "\n",
    "    for key, value in chart_data.items():\n",
    "        value_percentage = value / total_values * 100\n",
    "        rounded_value_percentage = round(value_percentage, 1)\n",
    "        chart_labels.append(key + ' - ' + str(rounded_value_percentage) + '%')\n",
    "\n",
    "# Plotting the percentege of bronze, silver and gold customers\n",
    "colors = ['#8B7355', '#C0C0C0', '#EEC900']\n",
    "customer_category = df.groupby(['customer_category']).count().sort_values(\n",
    "    by=['customer_id'], ascending=False)['customer_id']\n",
    "\n",
    "customer_category_labels = []\n",
    "customer_category_sizes = customer_category.values\n",
    "generate_legend_labels(customer_category, customer_category_labels)\n",
    "\n",
    "# Generates the pie chart\n",
    "customer_category_chart, customer_category_ax = plt.subplots()\n",
    "customer_category_ax.pie(customer_category_sizes,\n",
    "                         shadow=True, startangle=90, colors=colors)\n",
    "customer_category_ax.axis('equal')\n",
    "\n",
    "plt.legend(labels=customer_category_labels, loc='upper left',\n",
    "           bbox_to_anchor=(-0.25, 1.), fontsize=9)\n",
    "customer_category_ax.set_title(\n",
    "    'Percentage of each customer segment', fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Clustering algorithms\n",
    "\n",
    "# K means clustering\n",
    "\n",
    "# Importing libraries for K-means algorithm and performance measurments\n",
    "\n",
    "# Using elbow method to find out optimal k value with WCSS score\n",
    "# Creating empty list to insert values\n",
    "WCSS = []\n",
    "\n",
    "# Using a for function to fill the list with the WCSS scores\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(rfm_std)\n",
    "    WCSS.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the Elbow method graph\n",
    "plt.figure()\n",
    "plt.plot(range(1, 11), WCSS, marker='o', label='line with marker')\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Fitting K-Means to the dataset\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
    "# Use fit_predict to cluster the dataset\n",
    "km_clusters = kmeans.fit_predict(rfm_std)\n",
    "\n",
    "rfm_std_cl = pd.DataFrame(rfm_std, columns=rfm.columns, index=rfm.index)\n",
    "rfm_std_cl['cluster'] = km_clusters\n",
    "\n",
    "# Visualising the clusters (3 clusters according to the Elbow method)\n",
    "plt.scatter(rfm_std[km_clusters == 0, 0], rfm_std[km_clusters ==\n",
    "            0, 1], s=30, c='orange', label='Cluster 1')\n",
    "plt.scatter(rfm_std[km_clusters == 1, 0], rfm_std[km_clusters ==\n",
    "            1, 1], s=30, c='brown', label='Cluster 2')\n",
    "plt.scatter(rfm_std[km_clusters == 2, 0], rfm_std[km_clusters ==\n",
    "            2, 1], s=30, c='gray', label='Cluster 3')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[\n",
    "            :, 1], s=150, c='black', label='Centroids')\n",
    "plt.title('Clusters of customers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculating cluster validation metrics (silhouette score, calinski harabasz score, davies bouldin score)\n",
    "score_kemans_s = silhouette_score(rfm_std, kmeans.labels_, metric='euclidean')\n",
    "score_kemans_c = calinski_harabasz_score(rfm_std, kmeans.labels_)\n",
    "score_kemans_d = davies_bouldin_score(rfm_std, km_clusters)\n",
    "print('Silhouette Score: %.3f' % score_kemans_s)\n",
    "print('Calinski Harabasz Score: %.3f' % score_kemans_c)\n",
    "print('Davies Bouldin Score: %.3f' % score_kemans_d)\n",
    "\n",
    "# Hierarchical clustering\n",
    "\n",
    "# Plotting the dendrogram to find the optimal number of clusters\n",
    "plt.figure()\n",
    "dendrogram = sch.dendrogram(sch.linkage(rfm_std, method='ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "\n",
    "# Fitting Hierarchical Clustering to the dataset\n",
    "hc = AgglomerativeClustering(\n",
    "    n_clusters=4, affinity='euclidean', linkage='ward')\n",
    "hc_clusters = hc.fit_predict(rfm_std)\n",
    "\n",
    "# Visualising the clusters (4 clusters according to the Dendogram)\n",
    "plt.figure()\n",
    "plt.scatter(rfm_std[hc_clusters == 0, 0], rfm_std[hc_clusters == 0, 1], s = 30, c = 'orange', label = 'Cluster 1')\n",
    "plt.scatter(rfm_std[hc_clusters == 1, 0], rfm_std[hc_clusters == 1, 1], s = 30, c = 'brown', label = 'Cluster 2')\n",
    "plt.scatter(rfm_std[hc_clusters == 2, 0], rfm_std[hc_clusters == 2, 1], s = 30, c = 'gray', label = 'Cluster 3')\n",
    "plt.scatter(rfm_std[hc_clusters == 3, 0], rfm_std[hc_clusters == 3, 1], s = 30, c = 'yellow', label = 'Cluster 4')\n",
    "plt.title('Clusters of customers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculating cluster validation metrics\n",
    "score_AGclustering_s = silhouette_score(\n",
    "    rfm_std, hc.labels_, metric='euclidean')\n",
    "score_AGclustering_c = calinski_harabasz_score(rfm_std, hc.labels_,)\n",
    "score_AGclustering_d = davies_bouldin_score(rfm_std, hc_clusters)\n",
    "print('Silhouette Score: %.3f' % score_AGclustering_s)\n",
    "print('Calinski Harabasz Score: %.3f' % score_AGclustering_c)\n",
    "print('Davies Bouldin Score: %.3f' % score_AGclustering_d)\n",
    "\n",
    "# DBSCAN clustering\n",
    "\n",
    "# Since the DBSCAN code runs only with dataframes, first, we transformed the standardized array to a DataFrame\n",
    "rfm_std_df = pd.DataFrame(rfm_std, columns=rfm.columns)\n",
    "\n",
    "# Tuning the hyperparameters\n",
    "\n",
    "# Finding the 'n'\n",
    "min_sample = 2*len(rfm_std_df.columns)\n",
    "print(min_sample)  # equal to 6\n",
    "\n",
    "# Finding the epsilon\n",
    "# n = 7 because the first column will be of zeros\n",
    "nbrs = NearestNeighbors(n_neighbors=7).fit(rfm_std_df)\n",
    "# finding the k neighbors to each point\n",
    "neigh_dist, neigh_ind = nbrs.kneighbors(rfm_std_df)\n",
    "sort_neigh_dist = np.sort(neigh_dist, axis=0)  # sorting in ascending order\n",
    "\n",
    "# Plotting the graph to visualize which is the optimal epsilon value\n",
    "k_dist = sort_neigh_dist[:, 6]\n",
    "plt.plot(k_dist)\n",
    "# 0.4 chosen as it crosses the middle of the curve in the graph = optimal epsilon\n",
    "plt.axhline(y=0.4, linewidth=1, linestyle='dashed', color='k')\n",
    "plt.ylabel(\"k-NN distance\")\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.show()\n",
    "\n",
    "# Fitting the model\n",
    "dbscan_model = DBSCAN(eps=0.4, min_samples=6).fit(rfm_std_df)\n",
    "labels = dbscan_model.labels_\n",
    "rfm_std_df[\"LABEL\"] = labels\n",
    "rfm_std_df.head(10)\n",
    "\n",
    "# Plotting the graph to find the clusters\n",
    "g = sns.PairGrid(rfm_std_df, hue='LABEL', palette=\"Paired\")\n",
    "g = g.map(sns.scatterplot)\n",
    "print(g)\n",
    "\n",
    "# Fitting model and predicting clusters\n",
    "dbscan_clusters = dbscan_model.fit_predict(rfm_std_df)\n",
    "\n",
    "# Calculating cluster validation metrics\n",
    "score_dbsacn_s = silhouette_score(\n",
    "    rfm_std_df, dbscan_clusters, metric='euclidean')\n",
    "score_dbsacn_c = calinski_harabasz_score(rfm_std_df, dbscan_clusters)\n",
    "score_dbsacn_d = davies_bouldin_score(rfm_std_df, dbscan_clusters)\n",
    "print('Silhouette Score: %.3f' % score_dbsacn_s)\n",
    "print('Calinski Harabasz Score: %.3f' % score_dbsacn_c)\n",
    "print('Davies Bouldin Score: %.3f' % score_dbsacn_d)\n",
    "\n",
    "# Considering the validation metrics, K-means was our best model\n",
    "\n",
    "# Interpreting the clusters in K-means\n",
    "print('Size of each cluster: ')\n",
    "print(rfm_std_cl['cluster'].value_counts())\n",
    "\n",
    "# Separating clusters in different dataframes\n",
    "cluster0 = pd.DataFrame()\n",
    "cluster1 = pd.DataFrame()\n",
    "cluster2 = pd.DataFrame()\n",
    "\n",
    "for i in rfm_std_cl.index:\n",
    "    if rfm_std_cl['cluster'][i] == 0:\n",
    "        cluster0 = cluster0.append(rfm_std_cl.iloc[i])\n",
    "    elif rfm_std_cl['cluster'][i] == 1:\n",
    "        cluster1 = cluster1.append(rfm_std_cl.iloc[i])\n",
    "    elif rfm_std_cl['cluster'][i] == 2:\n",
    "        cluster2 = cluster2.append(rfm_std_cl.iloc[i])\n",
    "\n",
    "# Plotting the clusters in relation to other variables\n",
    "\n",
    "# 'New' customers = high recency, low frequency or monetary\n",
    "plt.figure()\n",
    "plt.subplot(311)\n",
    "parallel_coordinates(cluster0, 'cluster', color=('#556270'))\n",
    "plt.ylim(0, 15)\n",
    "# 'Lost' customers = moderate frequency and monetary, low recency\n",
    "plt.subplot(312)\n",
    "parallel_coordinates(cluster1, 'cluster', color=('#4ECDC4'))\n",
    "plt.ylim(0, 15)\n",
    "# 'Loyal' customers = high monetary value, above average frequency, moderate recency\n",
    "plt.subplot(313)\n",
    "parallel_coordinates(cluster2, 'cluster', color=('#C7F464'))\n",
    "plt.ylim(0, 15)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c2bd8370d9835a4a8706f1c8ec7394e0db6556a7b06b35ae493fb0b20d49980"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
